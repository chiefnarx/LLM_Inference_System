# services:
#   - type: web
#     name: llm-inference-api
#     env: docker  
#     plan: free
#     repo: https://github.com/chiefnarx/LLM_Inference_System
#     branch: main
#     autoDeploy: true


services:
  - type: web
    name: llm-inference
    env: docker
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn main:app --host 0.0.0.0 --port 10000
    pythonVersion: 3.10
